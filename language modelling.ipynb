{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Wars N-gram Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate some text based on the dialogues from the Star Wars scripts (episodes IV,V, and VI).\n",
    "\n",
    "All the information for this exercise was retrieved from the [Visualizing Star Wars Movie Scripts](https://github.com/gastonstat/StarWars) project.\n",
    "\n",
    "We start by reading all the dialogue lines from the scripts, which are labeled with the character speaking. We are only considering Luke, Leia, and Han Solo. We left Chewbacca out of the example for obvious reasons...\n",
    "\n",
    "We read all the lines of each character and combine them in one single string. We tokenize this string using the `WordPunctTokenizer` and use these tokens to create an NLTK Text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "import pandas\n",
    "\n",
    "wpt = WordPunctTokenizer()\n",
    "\n",
    "c3po_string = \"\"\n",
    "vader_string = \"\"\n",
    "solo_string = \"\"\n",
    "luke_string = \"\"\n",
    "leia_string = \"\"\n",
    "\n",
    "def read_lines(path):\n",
    "    lines = pandas.read_csv(path, delim_whitespace=True, error_bad_lines=False)\n",
    "    \n",
    "    c3po_lines = lines.loc[lines['Char'] == 'THREEPIO']['Text']\n",
    "    solo_lines = lines.loc[lines['Char'] == 'HAN']['Text']\n",
    "    vader_lines = lines.loc[lines['Char'] == 'VADER']['Text']\n",
    "    luke_lines = lines.loc[lines['Char'] == 'LUKE']['Text']\n",
    "    leia_lines = lines.loc[lines['Char'] == 'LEIA']['Text']\n",
    "    \n",
    "    \n",
    "    global vader_string, c3po_string, solo_string, luke_string, leia_string\n",
    "    c3po_string = c3po_string + \" \" + \" \".join(c3po_lines)\n",
    "    solo_string = solo_string + \" \" + \" \".join(solo_lines)\n",
    "    vader_string = vader_string + \" \" + \" \".join(vader_lines)\n",
    "    luke_string = luke_string + \" \" + \" \".join(luke_lines)\n",
    "    leia_string = leia_string + \" \" + \" \".join(leia_lines)\n",
    "    \n",
    "read_lines('files/SW_EpisodeIV.txt')\n",
    "read_lines('files/SW_EpisodeV.txt')\n",
    "read_lines('files/SW_EpisodeVI.txt')\n",
    "\n",
    "c3po_text = nltk.Text(wpt.tokenize(c3po_string))\n",
    "solo_text = nltk.Text(wpt.tokenize(solo_string))\n",
    "vader_text = nltk.Text(wpt.tokenize(vader_string))\n",
    "luke_text = nltk.Text(wpt.tokenize(luke_string))\n",
    "leia_text = nltk.Text(wpt.tokenize(leia_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these Text objects, we can proceed in the same way as in the previous examples, to generate texts. \n",
    "\n",
    "The following `generate_text_backoff` tries to generate a new word based on an N-gram proabability. If this fails, it tries the Tri-gram one and then the Bi-gram. If none of them are sucessful, it just stops. Recalling from the POS tagging session, this is known as a backoff strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_backoff(text, initialword, numwords):\n",
    "   \n",
    "    #ngrams\n",
    "    ngrams = list(nltk.ngrams(text, 4,  pad_right=True, pad_left=True))\n",
    "    ngram_pairs = (((w0, w1, w2), w3) for w0, w1, w2, w3 in ngrams)\n",
    "    cpdNgram = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(ngram_pairs), nltk.MLEProbDist)\n",
    "\n",
    "    #trigram \n",
    "    trigrams = list(nltk.ngrams(text, 3,  pad_right=True, pad_left=True))\n",
    "    trigram_pairs = (((w0, w1), w2) for w0, w1, w2 in trigrams)\n",
    "    cpd3gram = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(trigram_pairs), nltk.MLEProbDist)\n",
    "\n",
    "    #bigram\n",
    "    bigrams = list(nltk.ngrams(text, 2))\n",
    "    cpd2gram = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(bigrams), nltk.MLEProbDist)\n",
    "    \n",
    "    \n",
    "    word = initialword\n",
    "    for i in range(numwords):\n",
    "        #try n-gram\n",
    "        if (word[i],word[i+1], word[i+2]) in cpdNgram:\n",
    "            w = cpdNgram[(word[i],word[i+1], word[i+2])].max()\n",
    "        #try 3-gram\n",
    "        elif (word[i+1],word[i+2]) in cpd3gram:\n",
    "            w = cpd3gram[(word[i+1],word[i+2])].max()\n",
    "        #try 2-gram\n",
    "        elif word[i+2] in cpd2gram:\n",
    "            w = cpd2gram[word[i+2]].max()\n",
    "        #at least we tried...\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        word += [w]\n",
    "    \n",
    "    return \" \".join(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function ready, let's try to generate some texts, and see how they vary from one character to another, using by the same starting tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"C3PO: \" + generate_text_backoff(c3po_text, [\"It\", \"sure\", \"is\"], 25) + \"\\n\")\n",
    "print(\"Han Solo: \" + generate_text_backoff(solo_text, [\"Chewie\", \"come\", \"here\"], 25) + \"\\n\")\n",
    "print(\"Leia: \" + generate_text_backoff(leia_text, [\"It\", \"sure\", \"is\"], 25) + \"\\n\")\n",
    "print(\"Luke: \" + generate_text_backoff(luke_text, [\"It\", \"sure\", \"is\"], 25) + \"\\n\")\n",
    "print(\"Vader: \" + generate_text_backoff(vader_text, [\"I\", \"am\", \"your\"], 25) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
